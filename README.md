# FinanceMail â€” Classificador de E-mails (MVP)

O **FinanceMail** Ã© um MVP que classifica e-mails automaticamente como **produtivo** (demanda financeira/documental) ou **improdutivo** (social/conversa), calcula **confianÃ§a**, sugere uma **resposta pronta** e coleta **feedback** do usuÃ¡rio para evoluir o modelo.

> **Objetivo:** reduzir ruÃ­do em caixas de entrada e acelerar a triagem de mensagens ligadas a **pagamento, boleto, DANFE, nota fiscal, vencimento, valor, PIX**, etc.

---

## âœ¨ Funcionalidades

- **ClassificaÃ§Ã£o**: `produtivo` vs `improdutivo`
- **ConfianÃ§a**: score numÃ©rico + nÃ­vel (baixa / mÃ©dia / alta)
- **Resposta sugerida** (gerada por LLM)
- **Feedback do usuÃ¡rio** gravado em arquivo `.jsonl`
- **HistÃ³rico local (Ãºltimos 10)** no navegador (`localStorage`)
- **UI simples e â€œdemo-readyâ€** (React + Vite)

---

## ğŸ§  Como funciona (arquitetura)

Frontend (React + Vite)
â””â”€â”€ chama /api/*
(proxy do Vite evita CORS em dev)
â†“
Backend (FastAPI)
â”œâ”€â”€ Modelo local (TF-IDF + Logistic Regression)
â”‚ â””â”€â”€ categoria + confianÃ§a
â”œâ”€â”€ (opcional) LLM como â€œsegunda opiniÃ£oâ€
â”‚ â””â”€â”€ usado apenas quando a confianÃ§a Ã© baixa
â”œâ”€â”€ LLM gera a resposta sugerida
â””â”€â”€ Feedback â†’ grava em feedback/feedback.jsonl



---

## ğŸ§° Stack

### Backend
- FastAPI
- scikit-learn (TF-IDF + Logistic Regression)
- joblib (artefatos do modelo)
- HuggingFace Hub (`InferenceClient`) para LLM
- python-dotenv

### Frontend
- React + Vite
- Fetch API
- localStorage

---

## ğŸ“ Estrutura do projeto


FinanceMail/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py
â”‚ â”‚ â”œâ”€â”€ rl_model.py
â”‚ â”‚ â”œâ”€â”€ ai_client.py
â”‚ â”‚ â”œâ”€â”€ feedback_store.py
â”‚ â”‚ â”œâ”€â”€ text_rules.py
â”‚ â”‚ â””â”€â”€ sample_training_data.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ feedback/
â”‚ â”‚ â””â”€â”€ feedback.jsonl # gerado em runtime
â”‚ â””â”€â”€ artifacts/ # modelos e pipeline
â””â”€â”€ frontend/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ App.jsx
â”‚ â”œâ”€â”€ api.js
â”‚ â””â”€â”€ ...
â””â”€â”€ vite.config.js




---

## â–¶ï¸ Rodando localmente

### 1) Backend (FastAPI)

```bash
cd backend

python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt

# Crie o .env com o token do HuggingFace (necessÃ¡rio para o LLM)
echo "HF_TOKEN=SEU_TOKEN_AQUI" > .env

uvicorn app.main:app --reload --host 127.0.0.1 --port 8000

---

Backend

Health: http://127.0.0.1:8000/status

Swagger: http://127.0.0.1:8000/docs

2) Frontend (React + Vite)
cd ../frontend
npm install
npm run dev


Frontend

http://localhost:5173

Em desenvolvimento, o frontend chama o backend via proxy:
/api â†’ http://127.0.0.1:8000 (sem problemas de CORS).

ğŸ”‘ VariÃ¡veis de ambiente
Backend (backend/.env)

HF_TOKEN â€” token do HuggingFace para chamar o LLM
(geraÃ§Ã£o de resposta e/ou segunda opiniÃ£o conforme regra do backend)

Frontend (opcional)

VITE_API_BASE â€” por padrÃ£o o app usa "/api"

ğŸ”Œ API (principais endpoints)
POST /classificar

Classifica um texto de e-mail e devolve categoria, confianÃ§a e resposta sugerida.

Request

{
  "texto": "Segue boleto e DANFE para pagamento. Valor 430, vencimento 20/01."
}


Response (exemplo)

{
  "categoria": "produtivo",
  "confianca": 0.65,
  "fonte": "modelo",
  "resposta": "Prezado(a), ...",
  "categoria_modelo": "produtivo",
  "confianca_modelo": 0.65
}

POST /feedback

Grava um feedback do usuÃ¡rio para uso futuro.

Request

{
  "texto": "Teste UI",
  "previsto": "produtivo",
  "correto": "improdutivo"
}


Response

{ "ok": true }


Arquivo gerado

backend/feedback/feedback.jsonl

ğŸ§ª Teste rÃ¡pido (curl)
curl -X POST "http://127.0.0.1:8000/classificar" \
  -H "Content-Type: application/json" \
  -d '{"texto":"Feliz natal! Obrigado pelo atendimento :)"}'

curl -X POST "http://127.0.0.1:8000/feedback" \
  -H "Content-Type: application/json" \
  -d '{"texto":"Teste UI","previsto":"produtivo","correto":"improdutivo"}'

ğŸ“Œ DecisÃµes de produto (MVP)

Modelo local faz a classificaÃ§Ã£o (rÃ¡pido e barato)

LLM Ã© usado para:

gerar a resposta sugerida

atuar como â€œvalidaÃ§Ã£oâ€ apenas quando faz sentido (regra no backend)

Feedback Ã© salvo em .jsonl para evoluÃ§Ã£o e retreino posterior

âš ï¸ LimitaÃ§Ãµes (atuais)

Dataset inicial Ã© pequeno â†’ casos ambÃ­guos podem errar

Feedback ainda nÃ£o estÃ¡ automatizado em um pipeline de retreino

NÃ£o processa anexos (PDF/DANFE) â€” apenas texto (por enquanto)

ğŸ›£ï¸ Roadmap (prÃ³ximos passos)

Retreino periÃ³dico usando feedback.jsonl

Dashboard de mÃ©tricas (acurÃ¡cia, matriz de confusÃ£o, drift)

Upload de PDF/anexo e extraÃ§Ã£o de texto

Regras melhores para reduzir falsos positivos

Deploy pÃºblico (backend + frontend)

ğŸš€ Deploy (visÃ£o geral)
Backend (Render â€” Web Service)

VariÃ¡vel de ambiente: HF_TOKEN

Start command:

uvicorn app.main:app --host 0.0.0.0 --port 10000

Frontend (Vercel ou Render â€” Static Site)

Build:

npm run build


Output: dist

ğŸ“„ LicenÃ§a

Projeto de demonstraÃ§Ã£o / MVP.


---

Se quiser, no prÃ³ximo passo eu posso:
- adicionar **seÃ§Ã£o â€œImpacto / Resultadosâ€ (estilo recrutador)**  
- escrever o **roteiro do vÃ­deo de apresentaÃ§Ã£o (3â€“5 minutos)**  
- ou adaptar o README para **vaga de trainee/jÃºnior** com linguagem estratÃ©gica para RH e tech lead.
